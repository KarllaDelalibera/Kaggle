{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importação das libs utilizadas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.stattools import adfuller","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importação das bases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/walmart-recruiting-store-sales-forecasting/'\n\nstores = pd.read_csv(PATH + 'stores.csv')\nfeatures = pd.read_csv(PATH + 'features.csv.zip', compression='zip')\ntrain = pd.read_csv(PATH + 'train.csv.zip', compression='zip')\ntest = pd.read_csv(PATH + 'test.csv.zip', compression='zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Funções úteis \n\nO bloco abaixo apresenta algumas funções úteis que foram utilizadas no decorrer do código.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_missing_values(df, value_percent=False):\n    '''\n    Função responsável por verificar\n    quantidade de missing values em \n    um dataframe.\n    Entrada: Dataframe.\n    Retorno: Dataframe.\n    '''\n    aux = df.isnull().sum().to_frame(name='qtde_missing')\n    if value_percent:\n        aux['perc_qtde_missing'] = aux['qtde_missing'].apply(lambda x: x/df.shape[0])\n    return aux\n\n\n\ndef train_test_times_series(df, size_train):\n    '''\n    Função responsável por dividir uma base\n    em treino/teste respeitando a order da\n    serie.\n    Entrada: df->Dataframe, size_train->porcentagem da base de treino\n    Retorno: Dataframe\n    '''\n    \n    tamanho_base = len(df)\n    n_train = np.round(tamanho_base*size_train)\n    \n    treino = df.loc[0:n_train,:]\n    teste = df.loc[(n_train+1):tamanho_base,:]\n    return treino, teste\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explorando as bases de dados","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Base de dados: Stores\n\nNessa parte o objetivo é conhecer um pouco mais das lojas e responder algumas perguntas como:\n\n* Qual é a dimensão da base Store?\n* Alguma das variáveis que compõem a base Stores contém valores ausentes?\n* Quantas lojas são do Tipo A, B e C?\n* Em média quais são os tamanhos das lojas? Tenho alguma com tamanho negativo?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Qual é a dimensão da base Store?')\nprint('Temos {row} linhas e {col} colunas!'.format(row=stores.shape[0], col=stores.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Alguma das variáveis que compõem a base Stores contém valores ausentes?')\ncheck_missing_values(df=stores, value_percent=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Quantas lojas são do tipo A, B e C ?')\nqtde_lojas_tipo = stores.groupby('Type', as_index=False)['Store'].count()\nqtde_lojas_tipo.rename(columns={\"Store\": \"Qtde_Stores\"}, inplace=True)\nqtde_lojas_tipo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Em média quais são os tamanhos das lojas? Tenho alguma com tamanho negativo?')\nstores['Size'].describe().to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base de dados: Features\n\nNessa parte o objetivo é conhecer um pouco mais das lojas e responder algumas perguntas como:\n\n* Qual é a dimensão da base Fetures?\n* Alguma das variáveis que compõem a base Features contém valores ausentes?\n* Qual é o resumo estatístico de algumas variáveis?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Qual é a dimensão da base Fetures?')\nprint('Temos {row} linhas e {col} colunas!'.format(row=features.shape[0], col=features.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Alguma das variáveis que compõem a base Features contém valores ausentes?')\nprint('Pelo dataframe abaixo podemos peceber que mais de 50% da variável MarkDown 1-5 é representada por valores faltantes.')\ncheck_missing_values(df=features, value_percent=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Qual é o resumo estatístico de algumas variáveis?')\nfeatures.iloc[:,2:].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base de dados: Train\n\nNessa parte o objetivo é conhecer um pouco mais dos dados de train.\n\n* Qual é a dimensão da base Treino?\n* Alguma das variáveis que compõem a base Features contém valores ausentes?\n* Temos alguma venda negativa?\n* Quantos departamentos temos por loja?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Qual é a dimensão da base train?')\nprint('Temos {row} linhas e {col} colunas!'.format(row=train.shape[0], col=train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Alguma das variáveis que compõem a base train contém valores ausentes?')\ncheck_missing_values(df=train, value_percent=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Temos alguma venda negativa?')\nprint('Pela dataframe abaixo, podemos peceber que existe vendas semanais com valores negativos.')\ntrain.iloc[:,3:].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Quantos departamentos temos por loja?')\ntrain.groupby('Store')['Dept'].nunique().to_frame().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base de dados: Test\n\nNessa parte o objetivo é conhecer um pouco mais dos dados de test.\n\n* Qual é a dimensão da base Test?\n* Alguma das variáveis que compõem a base Features contém valores ausentes?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Qual é a dimensão da base test?')\nprint('Temos {row} linhas e {col} colunas!'.format(row=test.shape[0], col=test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Alguma das variáveis que compõem a base test contém valores ausentes?')\ncheck_missing_values(df=test, value_percent=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Construção da base ABT (Analytical Base Table)\n\nApós a pequena exploração nos dados, o próximo passo é a construção da nossa ABT. Isto é, a base que será usada para criação do modelo.\n\nNessa parte iremos realizar algumas ações sobre os seguintes pontos:\n\n* Junção das bases\n* Valores missing na base Features\n* Valores negativos de vendas na base Train\n* Criação de novas variáveis e algumas transformações\n* Correlação entre as variáveis\n* Normalização dos dados\n* Verificação de estacionariedade nas séries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Junção das bases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full = train.merge(features, on=['Store', 'Date'], how='inner').merge(stores, on=['Store'], how='inner')\ntrain_full.drop(columns=['IsHoliday_y'], inplace=True)\ntrain_full.rename(columns={'IsHoliday_x':'IsHoliday'}, inplace=True)\n\nprint('Temos {row} linhas e {col} colunas!'.format(row=train_full.shape[0], col=train_full.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = test.merge(features, on=['Store', 'Date'], how='inner').merge(stores, on=['Store'], how='inner')\ntest_full.drop(columns=['IsHoliday_y'], inplace=True)\ntest_full.rename(columns={'IsHoliday_x':'IsHoliday'}, inplace=True)\n\nprint('Temos {row} linhas e {col} colunas!'.format(row=test_full.shape[0], col=test_full.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Valores missing oriundos da base Features\n\nComo o percentual de valores missing nas variáveis MarkDown 1-5 são altos, não será realizado nenhum tratamento nelas, uma vez que elas serão desconsideras da base em um primeiro momento.\n\nJá as variáveis CPI e Unemployment utilizarei a mediana (agrupando por loja, pois cada loja percente a uma região) para imputação de dados.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Removendo as variáveis MarkDown 1-5 da base train_abt e test_abt')\ntrain_full.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5'], inplace=True)\ntest_full.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Aplicando a mediana como imputação de dados para as variáveis: CPI e Unemployment ')\ntest_full['CPI'] = test_full['CPI'].fillna(test_full.groupby('Store')['CPI'].transform('median'))\ntest_full['Unemployment'] = test_full['Unemployment'].fillna(test_full.groupby('Store')['Unemployment'].transform('median'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Verificando novamente se temos valores ausentes na base train')\ncheck_missing_values(df=train_full, value_percent=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Verificando novamente se temos valores ausentes na base test')\ncheck_missing_values(df=test_full, value_percent=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Valores negativos de vendas na base Train\n\nPara tratar o problema de vendas semanais negativas, resolvi trabalhar com faixas de intevalos. Sei que essas vendas não podem ser negativas, então o intervalo para a variável Weekly_Sales seria de 0 até a maior venda.\n\nComo descrito na documentação do numpy: Dado um intervalo, os valores fora do intervalo são cortados nas bordas do intervalo. Por exemplo, se um intervalo de for especificado ([0,1]), valores menores que 0 se tornarão 0 e valores maiores que 1 se tornarão 1.\n\nhttps://numpy.org/doc/stable/reference/generated/numpy.clip.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Aplicando tratamento de valores negativos na base')\ntrain_full['Weekly_Sales'] = train_full.Weekly_Sales.clip(0, train_full.Weekly_Sales.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"É importante ressaltar que substituir por zero as vendas negativas não é uma boa prática. Uma outra ação (que demandariam mais tempo) que poderia ser aplicada é: devsenvolver um modelo de ML para realizar uma inputação de dados.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Criação de novas variáveis e alguns transformações\n\nNessa parte, será criado uma variável a partir de outra variável já existente, e transformaremos algumas variáveis categoricas em númericas. \n\n* Criaremos uma variável referente ao dia da semana, a partir da variável Date.\n\nSegundo a documentação do pandas: Segunda-feira = 0, domingo = 6.\n\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.dayofweek.html\n\n* Transformação das variáveis IsHoliday e Type em númericas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Transformando a variável Date em datetime')\ntrain_full['Date'] = pd.to_datetime(train_full['Date'])\ntest_full ['Date'] = pd.to_datetime(test_full['Date'])\n\nprint('Criação da variavel dia da semana')\n\ntrain_full['day_of_week'] = train_full['Date'].dt.dayofweek\ntest_full['day_of_week'] = test_full['Date'].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Transformação das variáveis IsHoliday e Type em númericas')\n\ntrain_full = pd.get_dummies(train_full, columns=['Type'])\ntrain_full['IsHoliday'] = train_full['IsHoliday'].astype(int)\n\n\ntest_full = pd.get_dummies(test_full, columns=['Type'])\ntest_full['IsHoliday'] = test_full['IsHoliday'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlação entre as variáveis\n\nApresentaremos uma matriz de correlação para verificar se existe alguma correlação entre as variáveis da base de treino.\nEssa etapa é de grande valia, uma vez que usar variáveis altamente correlacionados acaba não fazendo sentido, pois teremos uma ou mais variáveis fornecendo as mesmas informações quando inseridos em um modelo de previsão. \n\nSendo assim, é importante identificar essa correlação e se necessário remover algumas variáveis.\n\n* Interpretação da correlação\n\n * Um valor próximo de +1 indica: um alto grau de associação entre as duas variáveis. Isto é, a medida que uma variável cresce a outra cresce também.\n \n * Um valor próximo de -1 indica: um alto grau de associação entre as duas variáveis. Isto é, a medida que uma variável cresce a outra decresce.\n \n * Um valor próximo a 0 indica: uma fraca ou ausente associação entre as variáveis.\n \n* Para o cálculo da correção utilizarei o método de Sperman (não parámetrico), uma vez que o método usual de Pearson tem como pressuposto normalidade dos dados. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtrando apenas as variáveis quantitativas\nvariaveis_corr = ['Weekly_Sales', 'Temperature','Fuel_Price', 'CPI', 'Unemployment', 'Size']\ncorrelacao_train_full = train_full[variaveis_corr].corr(method = 'spearman')\nplt.figure(figsize=(15, 10))\nsns.heatmap(correlacao_train_full, annot=True)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A princípio nenhuma variável precisará ser removida.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Normalização dos dados\n\nSerá aplicado a normalização em algumas variáveis da base para corrigir problemas de escalas diferentes e também para uma melhor performance do modelo.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"colunas_trans_scaler = ['Temperature','Fuel_Price', 'CPI', 'Unemployment', 'Size']\ntrans_scaler = StandardScaler().fit(train_full[colunas_trans_scaler])\n\ntrain_full[colunas_trans_scaler] = trans_scaler.transform(train_full[colunas_trans_scaler])\ntest_full[colunas_trans_scaler] = trans_scaler.transform(test_full[colunas_trans_scaler])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Verificação de estacionariedade nas séries\n\nFormalmente dizendo, uma série é definida como estacionária quando ela se desenvolve no tempo aleatoriamente ao redor de uma média constante. (Veja http://www.portalaction.com.br/series-temporais/11-estacionariedade#:~:text=Uma%20s%C3%A9rie%20temporal%20%C3%A9%20dita,estacionariedade%2C%20por%20exemplo%2C%20tend%C3%AAncia.)\nDe maneira resumida, as as propriedas estatísticas da série não mudam conforme o tempo.\n\nIsso é importante ser verificado, pois caso a série não seja estacionária e abordarmos um modelos de séries temporais (ex: ARIMA, SARIMAX) um dos pressupostos desses modelos é que a série seja estacionária.\n\nComo também, se abordarmos modelos de Machine Leaning, alguns parâmetros como sazonalidade podem ser mais facilmente modelados, outros como a tendência podem ser mais dificeis para o modelo entender.\n\n* Teste de Raíz unitária (teste de Dickey-Fuller)\n\nPara verificação da estacionariedade das séries, utilizaremos o teste de Raíz unitária. O teste tem como hipótese nula que os dados não são estacionários. Considerando um nível de confiança de 95%, temos que:\n\n * Se o p-valor do teste for maior que 0.05 não rejeitamos a hipótese nula, isto é, a série é não estacionária.\n * Se o p-valor do teste for menor que 0.05 rejeitamos a hipótese nula, isto é, a série é estacionária.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_dept = []\nfor i in range(0,len(list(train_full['Store'].unique()))):\n    for j in list(train_full.loc[train_full.Store == i, 'Dept'].unique()):\n        aux = train_full.loc[(train_full.Store == i) & (train_full.Dept == j)]\n        try:\n            p_value = adfuller(aux.Weekly_Sales)[1]\n            if p_value > 0.05:\n                store_dept.append((i,j, p_value))\n        except:\n            continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_serie_nestac são as séries não estacionárias segundo teste da Raíz unitária \n\ndf_serie_nestac = pd.DataFrame(store_dept, columns = ['store','dept','p_value'])\ndf_serie_nestac['chave'] = df_serie_nestac['store'].astype(str) + '_' + df_serie_nestac['dept'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_serie_nestac.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pelo dataframe acima (df_serie_nestac), podemos ver que algumas séries não satisfazem a propriedade de estacionariedade.\n\nInicialmente vou modelar a série sem nenhuma tratativa para esse problema, a fim de verificarmos se o modelo de ML conseguirá modelar essas tendências (sazonais ou não).\n\nEm um segundo momento, podemos testar dois modelos de ML dividindo as séries em dois grandes grupos: as que são estacionárias e as que não são estacionárias.\n\nO meu objetivo ao propor esse \"agrupamento\" de séries, é colocar as séries que possuem as mesmas propriedades/se comportam de maneira parecida uma para cada modelo.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Modelagem - Considerando todos as séries\n\nConstruindo nossa ABT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Não consideraremos Type_A,Type_B e Type_C, uma vez que eles já estão representados na variável Size\n\nfeatures_abt = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature', \n                'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'day_of_week']\n\ntrain_abt = train_full[features_abt]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Divisão da base em treino e teste para validação da qualidade do modelo\n\nUm ponto importante quando trabalhamos com modelagem, é obtermos algumas medidas para verificarmos a precisão de um modelo, isto é, medidas que podem nos ajudar a verificar o quão bom é um modelo. \n\nGeralmente para obtermos esses tipos de medidas precisamos dos valores previstos pelo modelo e dos valores reais observados. Com base nisso, uma maneira de obtermos essas medidas é dividirmos a série de dados em bases de treinamento e teste, respeitando a ordem da série (para modelos temporais). Assim, o modelo pode ser construído no conjunto de dados de treinamento e as previsões podem ser feitas e avaliadas no conjunto de dados de teste.\n\nEssa divisão pode ser feita selecionando um ponto de divisão arbitrário na série de observações e criando dois novos conjuntos de dados. Assim, se tivermos uma série de 100 observações podemos usar uma divisão do tipo: 70 a 30. Em que as 70 primeiras observações faram parte do conjunto de dados de treinamento e as 30 seguintes observações faram parte do conjunto de dados de teste. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Modelando","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_abt_sort = train_abt.sort_values(by=['Store','Dept','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para este problema, dividirei a base em 90% para treino do modelo e 10% para teste. Isto é, 90% dos primeiros valores da série ordenada será para treinamento e os 10% restantes será para teste.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"treino, teste = train_test_times_series(train_abt_sort, 0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dimensão da base de treino é {shape}'.format(shape=treino.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dimensão da base de teste é {shape}'.format(shape=teste.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Store', 'Dept', 'IsHoliday', 'Temperature',\n           'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'day_of_week']\n\ntarget = 'Weekly_Sales'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test = treino[features], teste[features]\ny_train, y_test = treino[target], teste[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelo considerado: ExtraTreesRegressor\n\n\nPara a modelagem de uma série temporal, podemos encontrar na literatura modelos como:\n\n* Suavização Exponencial\n* SARIMA, SARIMAX \n\nTodavia, também podemos utilizar modelos regressores comuns (respeitando os conceitos de uma série temporal) como:\n\n* Árvores de Decisão (RandomForest, ExtraTreesRegressor)\n* Redes neurais simples (MLP)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"etr = ExtraTreesRegressor(n_estimators=100, random_state=42)\netr.fit(X_train, y_train)\nprint('R² da ExtraTreesRegressor na base teste foi: {0:.2f}'.format(etr.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelagem - Considerando dois grupos de séries\n\nConstruindo nossa ABT levando em considerando dois grandes grupos: séries estacionárias e séries não estacionárias","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Nessa parte dividimos a base de treino em dois grandes grupos: as que são estacionárias (series_estac) e as que não são (series_n_estac).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full['chave'] = train_full['Store'].astype(str) + '_' + train_full['Dept'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_estac = train_full.loc[~train_full.chave.isin(df_serie_nestac['chave'])]\n\nseries_n_estac = train_full.loc[train_full.chave.isin(df_serie_nestac['chave'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Construindo nossa ABT tanto para as séries estacionárias quanto para as não estacionárias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_abt = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature', \n                'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'day_of_week']\n\ntrain_abt_est = series_estac[features_abt]\ntrain_abt_n_est = series_n_estac[features_abt]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelando as séries estacionárias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_abt_est_sort = train_abt_est.sort_values(by=['Store','Dept','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino_est, teste_est = train_test_times_series(train_abt_est_sort, 0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Store', 'Dept', 'IsHoliday', 'Temperature',\n           'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'day_of_week']\n\ntarget = 'Weekly_Sales'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_est, X_test_est = treino_est[features], teste_est[features]\ny_train_est, y_test_est = treino_est[target], teste_est[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etr_est = ExtraTreesRegressor(n_estimators=100, random_state=42)\netr_est.fit(X_train_est, y_train_est)\nprint('R² da ExtraTreesRegressor na base teste foi: {0:.2f}'.format(etr_est.score(X_test_est, y_test_est)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelando as séries não estacionárias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_abt_nest_sort = train_abt_n_est.sort_values(by=['Store','Dept','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aplicando a diferenciação de ordem 1\n\ntrain_abt_nest_sort.loc[:,'diff_Weekly_Sales'] = train_abt_nest_sort.groupby(['Store','Dept'])['Weekly_Sales'].diff(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_abt_nest_sort.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino_nest, teste_nest = train_test_times_series(train_abt_nest_sort, 0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Store', 'Dept', 'IsHoliday', 'Temperature',\n            'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'day_of_week']\n\ntarget = 'diff_Weekly_Sales'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_nest, X_test_nest = treino_nest[features], teste_nest[features]\ny_train_nest, y_test_nest = treino_nest[target], teste_nest[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etr_nest = ExtraTreesRegressor(n_estimators=100, random_state=42)\n\netr_nest.fit(X_train_nest, y_train_nest)\nprint('R² da ExtraTreesRegressor na base teste foi: {0:.2f}'.format(etr_nest.score(X_test_nest, y_test_nest)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelo final\n\nComparando os resultados obtidos, podemos perceber que a primeira versão de modelagem (Considerando todos as séries) apresentou um melhor desempenho (um R² de 0,86).\n\nPodemos perceber também que o R² obtido na modelagem das séries não estacionárias foi bem baixo. Uma hipótese que surge, é que provavelmente a tendência presente nas séries pode ser explicada pelas variéveis.\n\n\nModelagem escolhida: Modelagem - Considerando todos as séries\n\n\nPróximos passos:\n\n* Treinar novamente o modelo considerando a base train completamente (sem divisão treino/teste)\n* Aplicar o modelo na base test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Store', 'Dept', 'IsHoliday', 'Temperature',\n            'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'day_of_week']\n\ntarget = 'Weekly_Sales'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = train_abt_sort[features], train_abt_sort[target]\n\netr_model = ExtraTreesRegressor(n_estimators=100, random_state=42)\netr_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_test_abt = ['Store', 'Dept', 'Date', 'IsHoliday', 'Temperature', \n                     'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'day_of_week']\n\ntest_abt = test_full[features_test_abt]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_abt_sort = test_abt.sort_values(['Store','Dept','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_abt_sort['pred_ETR'] = etr_model.predict(test_abt_sort[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_abt_sort.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['Id'] = test_abt_sort['Store'].astype(str) + '_' + test_abt_sort['Dept'].astype(str) + '_' + test_abt_sort['Date'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['Weekly_Sales'] = test_abt_sort['pred_ETR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}